{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a4a449",
   "metadata": {},
   "source": [
    "## 教程5:聚合\n",
    "在本教程中，将重写Pytorch Geometric 中 GIN卷积模块的聚合方法，实现以下方法：  \n",
    "* 主要领域聚合（PNA）  \n",
    "* 学习聚合函数（LAF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87766d27",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch_scatter/_version_cpu.so, 0x0006): symbol not found in flat namespace '__ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch_scatter/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m spec \u001b[38;5;241m=\u001b[39m cuda_spec \u001b[38;5;129;01mor\u001b[39;00m cpu_spec\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBUILD_DOCS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibrary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mosp\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    928\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch_scatter/_version_cpu.so, 0x0006): symbol not found in flat namespace '__ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474183e",
   "metadata": {},
   "source": [
    "### 消息传递类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3871c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce6b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUPPORTS_FUSED_EDGE_INDEX',\n",
       " 'T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_check_input',\n",
       " '_collect',\n",
       " '_compiled_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_edge_updater_signature',\n",
       " '_get_name',\n",
       " '_get_propagate_signature',\n",
       " '_index_select',\n",
       " '_index_select_safe',\n",
       " '_lift',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_jittable_templates',\n",
       " '_set_size',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'aggregate',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decomposed_layers',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'edge_update',\n",
       " 'edge_updater',\n",
       " 'eval',\n",
       " 'explain',\n",
       " 'explain_message',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'jittable',\n",
       " 'load_state_dict',\n",
       " 'message',\n",
       " 'message_and_aggregate',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'propagate',\n",
       " 'register_aggregate_forward_hook',\n",
       " 'register_aggregate_forward_pre_hook',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_edge_update_forward_hook',\n",
       " 'register_edge_update_forward_pre_hook',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_message_and_aggregate_forward_hook',\n",
       " 'register_message_and_aggregate_forward_pre_hook',\n",
       " 'register_message_forward_hook',\n",
       " 'register_message_forward_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_propagate_forward_hook',\n",
       " 'register_propagate_forward_pre_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'special_args',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'update',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(MessagePassing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed88e5",
   "metadata": {},
   "source": [
    "对聚合方法感兴趣或者如果使用稀疏邻接矩阵，对消息和聚合方法感兴趣，我们构建了自定义的卷积类，该类扩展了GINConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3fd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter, Module, Sigmoid\n",
    "import torch    # 用于图数据的散列聚合操作\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### 定义LAFLayer的基础结构，处理权重初始化、设备管理、关键张量的预定义\n",
    "class AbstractLAFLayer(Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AbstractLAFLayer, self).__init__()\n",
    "\n",
    "        # --- 设备管理模块 ---\n",
    "        assert 'units' in kwargs or 'weights' in kwargs # 必须提供units或weights参数\n",
    "        if 'device' in kwargs.keys():\n",
    "            self.device = kwargs['device']  # 指定设备(CPU/GPU)\n",
    "        else:                               # 自动选择可用设备\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.ngpus = torch.cuda.device_count()  # 可用GPU数量\n",
    "\n",
    "        # --- 权重初始化模块 ---\n",
    "        if 'kernel_initializer' in kwargs.keys():\n",
    "            # 检测初始化方法是否合法\n",
    "            assert kwargs['kernel_initializer'] in [\n",
    "                'random_normal', 'glorot_normal', 'he_normal', \n",
    "                'random_uniform', 'glorot_uniform', 'he_uniform'\n",
    "            ]\n",
    "            self.kernel_initializer = kwargs['kernel_initializer']\n",
    "        else:\n",
    "            self.kernel_initializer = 'random_normal'  # 默认初始化方法\n",
    "        \n",
    "        # --- 权重创建逻辑 ---\n",
    "        if 'weights' in kwargs.keys():\n",
    "            # 直接使用外部提供的权重\n",
    "            self.weights = Parameter(kwargs['weights'].to(self.device), requires_grad=True)\n",
    "            self.units = self.weights.shape[1]  # 根据权重形状确定units\n",
    "        else:\n",
    "            # 指定units创建随机权重\n",
    "            self.units = kwargs['units']\n",
    "            params = torch.empty(12, self.units, device=self.device)   # 12xN权重矩阵\n",
    "\n",
    "            # 根据初始化策略填充权重\n",
    "            if self.kernel_initializer == 'random_normal':      # 标准正态分布初始化\n",
    "                torch.nn.init.normal_(params)\n",
    "            elif self.kernel_initializer == 'glorot_normal':    # Glorot正态分布初始化\n",
    "                torch.nn.init.xavier_normal_(params)\n",
    "            elif self.kernel_initializer == 'he_normal':        # He正态分布初始化\n",
    "                torch.nn.init.kaiming_normal_(params)\n",
    "            elif self.kernel_initializer == 'random_uniform':   # 均匀分布初始化\n",
    "                torch.nn.init.uniform_(params)\n",
    "            elif self.kernel_initializer == 'glorot_uniform':   # Glorot均匀分布初始化\n",
    "                torch.nn.init.xavier_uniform_(params)\n",
    "            elif self.kernel_initializer == 'he_uniform':       # He均匀分布初始化\n",
    "                torch.nn.init.kaiming_uniform_(params)\n",
    "            self.weights = Parameter(params, requires_grad=True)  # 注册为可训练参数\n",
    "        \n",
    "        # --- 预定义关键张量 ---\n",
    "        e = torch.tensor([1, -1, 1, -1], dtype=torch.float32, device=self.device)   \n",
    "        self.e = Parameter(e, requires_grad=False) # 用于符号变换的张量\n",
    "\n",
    "        num_idx = torch.tensor([1,1,0,0], dtype=torch.float32, device=self.device).view(1,1,-1,1)\n",
    "        self.num_idx = Parameter(num_idx, requires_grad=False) # 分子索引掩码\n",
    "\n",
    "        den_idx = torch.tensor([0,0,1,1], dtype=torch.float32, device=self.device).view(1,1,-1,1)\n",
    "        self.den_idx = Parameter(den_idx, requires_grad=False) # 分母索引掩码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3cb2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 实现核心的LAF操作，用于图数据的特征聚合\n",
    "class LAFLayer(AbstractLAFLayer):\n",
    "    def __init__(self, eps=1e-7,**kwargs):\n",
    "        super(LAFLayer, self).__init__(**kwargs)  # 调用父类初始化\n",
    "        self.eps = eps                            # 防止除零的小常数\n",
    "\n",
    "    def forward(self, data, index, dim=0, **kwargs):\n",
    "        \"\"\"\n",
    "        前向传播实现：\n",
    "        Args:\n",
    "            data: 输入特征张量 [N, F]\n",
    "            index: 图节点索引 [E] （表示边的连接关系）\n",
    "            dim: 聚合维度，默认0\n",
    "        \"\"\"\n",
    "        eps = self.eps\n",
    "        sup = 1.0 - eps   # 上界，防止数值不稳定\n",
    "\n",
    "        # --- 数据预处理 ---\n",
    "        x = torch.clamp(data, min=-sup, max=sup)  # 限制输入范围\n",
    "        x = torch.unsqueeze(x, -1)  # 扩展维度 [N, F, 1]\n",
    "        e = self.e.view(1,1,-1)  # 符号变换张量 [1, 1, 4]\n",
    "\n",
    "        # --- 指数变换阶段 ---\n",
    "        #  计算： (1-e)/2 * x*e -> [0.5, 1.5, 0.5, 1.5] 区间\n",
    "        exps = (1. - e)/2 + x*e\n",
    "        exps = torch.unsqueeze(exps, -1)  # [N, F, 4, 1]\n",
    "        # 应用科学系参数权重\n",
    "        exps = torch.pow(exps, torch.relu(self.weights[0:4]))  # [N, F, 4, units]\n",
    "\n",
    "        # --- 聚合阶段 ---\n",
    "        # 按照index对边特征求和聚合（图卷积核心）\n",
    "        scatter = torch_scatter.scatter_add(exps, index.view(-1), dim=dim)\n",
    "        scatter = torch.clamp(scatter, eps)  # 防止数值不稳定   \n",
    "\n",
    "        # --- 特征变换阶段 ---\n",
    "        # 应用第二指数权重（5-8个参数）\n",
    "        sqrt = torch.pow(scatter, torch.relu(self.weights[4:8]))  # [N, F, 4, units]\n",
    "        # 重新第三组权重\n",
    "        alpha_beta = self.weights[8:12].view(1,1,4,-1)\n",
    "        terms = sqrt * alpha_beta  # 加权特征 [N, F, 4, units]\n",
    "\n",
    "        # --- 分子分母计算 ---\n",
    "        num = torch.sum(terms * self.num_idx, dim=2)  # 分子：索引0+1位置求和\n",
    "        den = torch.sum(terms * self.den_idx, dim=2)  # 分母：索引2+3位置求和\n",
    "\n",
    "        # 防止分母为0的技巧\n",
    "        multiplier = 2.0*torch.clamp(torch.sign(den), min=0.0) - 1.0 # 生成+1/-1掩码\n",
    "        den = torch.where((den<eps) & (den > -eps), multiplier*eps, den)  # 使用+/-eps替换0值\n",
    "\n",
    "        # 最终输出 = 分子/分母\n",
    "        res = num / den\n",
    "        return res\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e2e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义GINLAFConv 图卷积层，结合GIN和LAF机制\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "class GINLAFConv(GINConv):\n",
    "    def __init__(self, nn, units=1, node_dim=32, **kwargs):\n",
    "        super(GINLAFConv, self).__init__(nn, **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_normal')\n",
    "        self.mlp = Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        x = torch.sigmoid(inputs)                   # 归一化\n",
    "        x = self.laf(x, index)                      # LAF聚合\n",
    "        x = x.view((-1, self.dim*self.units))       # 特征重塑\n",
    "        x = self.mlp(x)                             # 线性变换\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2655d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNA聚合\n",
    "class GINPNAConv(GINConv):\n",
    "    def __init__(self, nn, node_dim=32, **kwargs):\n",
    "        super(GINPNAConv, self).__init__(nn, **kwargs)\n",
    "        self.mlp = torch.nn.Linear(node_dim*12, node_dim)\n",
    "        self.delta = 2.5749\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        ## --- 步骤1： 基础聚合函数计算 ---\n",
    "        sums = torch_scatter.scatter_add(inputs, index, dim=0)      # 1. 求和聚合\n",
    "        maxs = torch_scatter.scatter_max(inputs, index, dim=0)[0]   # 2. 最大聚合\n",
    "        means = torch_scatter.scatter_mean(inputs, index, dim=0)    # 3. 均值聚合\n",
    "        var = torch.relu(torch_scatter.scatter_mean(inputs ** 2, index, dim=0) - means ** 2) # 4. 方差聚合\n",
    "\n",
    "        # 存储4种基础聚合结果\n",
    "        aggrs = [sums, maxs, means, var]\n",
    "\n",
    "        ## --- 步骤2： 节点度信息计算 ---\n",
    "        # 计算每个节点的度\n",
    "        c_idx = index.bincount().float().view(-1, 1) # [num_nodes, 1]\n",
    "        l_idx = torch.log(c_idx+1.)     # 度的对数平滑\n",
    "\n",
    "        ## --- 步骤3： 度依赖归一化 ---\n",
    "        amplification_scaler = [c_idx/self.delta*a for a in aggrs]  # 放大归一化：强化高度节点的聚合信号\n",
    "        attenuation_scaler = [self.delta/c_idx*a for a in aggrs]    # 衰减归一化：抑制高度节点的聚合信号\n",
    "\n",
    "        ## --- 步骤4： 特征组合 ---\n",
    "        # 拼接所有特征：4基础 + 4放大 + 4衰减 = 12种特征视角\n",
    "        combinations = torch.cat(\n",
    "            aggrs + amplification_scaler + attenuation_scaler,\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        ## --- 步骤5： 特征压缩 ---\n",
    "        x = self.mlp(combinations)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab11378",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747920e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing, SAGEConv, GINConv, global_add_pool\n",
    "import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import os.path as osp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d3921",
   "metadata": {},
   "source": [
    "#### LAFNET模型\n",
    "核心功能：  \n",
    "一个基于GINLAFConv的5层图神经网络，专为图分类任务设计：  \n",
    "* 使用LAF聚合替代传统求和操作  \n",
    "* 通过BatchNorm和Dropout防止过拟合  \n",
    "* 最终用全局加和池化将节点特征聚合为图级表示  \n",
    "____  \n",
    "核心设计原理：  \n",
    "* 层次化特征提取：  \n",
    "每层捕获不同跳数的邻居信息（1层=1跳邻居，5层=5跳邻居）\n",
    "* 全局池化的选择-global_add_pool  \n",
    "1. 保持置换不变形：图结构无序，加操作满足对称性\n",
    "2. 比 global_mean_pool 保留更多信息  \n",
    "3. 比 global_max_pool 保留更多细节(避免弱信号丢失)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8faadac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAFNET(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_class, **kargus):\n",
    "        super(LAFNET, self).__init__()\n",
    "\n",
    "        ## --- 模型超参数定义 ---\n",
    "        self.num_features = num_features        # 输入特征维度\n",
    "        self.num_class = num_class              # 输出分类种类\n",
    "        self.dim = 32                           # 隐藏层维度\n",
    "        self.units =3                           # LAF 扩展单元数（每个特征生成3个视角）\n",
    "\n",
    "        ## --- 第1层： 输入特征处理 ---\n",
    "        nn1 = Sequential(Linear(self.num_features, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv1 = GINLAFConv(nn1, units=self.units, node_dim=self.num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(self.dim)       # 确保 LAF 输入维度匹配\n",
    "        \n",
    "        ## --- 第2层： 隐层处理（结构相同，维度传递）---\n",
    "        nn2 = Sequential(Linear(self.dim, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv2 = GINLAFConv(nn2, units=self.units, node_dim=self.dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(self.dim, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv3 = GINLAFConv(nn3, units=self.units, node_dim=self.dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(self.dim) \n",
    "\n",
    "        nn4 = Sequential(Linear(self.dim, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv4 = GINLAFConv(nn4, units=self.units, node_dim=self.dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(self.dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(self.dim, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv5 = GINLAFConv(nn5, units=self.units, node_dim=self.dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(self.dim)\n",
    "        \n",
    "        ## --- 全连接层：图级分类 ---\n",
    "        self.fc1 = Linear(self.dim, self.dim)       # 隐藏层\n",
    "        self.fc2 = Linear(self.dim, self.num_class)      # 输出层\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        前向传递\n",
    "        1. x: 节点特征：[num_nodes, num_features]\n",
    "        2. edge_index: 边索引 [2, num_edges]\n",
    "        3. batch： 节点所属图索引 [num_nodes]\n",
    "        \"\"\"\n",
    "        ## --- 第1层：GINLAFConv ---\n",
    "        # 输入：x=[N,F],edge_index=[2, E] -> 输出：[N, dim]; 作用：初始特征转换+LAF聚合\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)     # [N, dim] - 维度归一化\n",
    "        ## --- 第2-5层：GINLAFConv ---  : 作用 多层特征提取 + 聚合， 捕获高阶结构信息\n",
    "        x = F.relu(self.conv2(x, edge_index))            # 作用：对conv1特征转化 + LAF聚合\n",
    "        x = self.bn2(x)     # [N, dim] - 维度归一化\n",
    "        x = F.relu(self.conv3(x, edge_index))            # 作用：对conv2特征转化 + LAF聚合\n",
    "        x = self.bn3(x)     # [N, dim] - 维度归一化\n",
    "        x = F.relu(self.conv4(x, edge_index))            # 作用：对conv3特征转化 + LAF聚合\n",
    "        x = self.bn4(x)     # [N, dim] - 维度归一化\n",
    "        x = F.relu(self.conv5(x, edge_index))            # 作用：对conv4特征转化 + LAF聚合\n",
    "        x = self.bn5(x)     # [N, dim] - 维度归一化\n",
    "        ## --- 全局图池化 ---\n",
    "        # 输入：x=[N,dim], batch=[N] -> 输出：[num_graphs, dim]; 作用：将节点特征聚合为图级表示\n",
    "        x = global_add_pool(x, batch)\n",
    "        ## --- 全连接分类器 ---\n",
    "        x = F.relu(self.fc1(x))     # [G, dim] -> [G, dim]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)             # [G, dim] -> [G, num_class]\n",
    "        return F.log_softmax(x, dim=-1) # [G, num_class] -> [G, num_class] 对每个节点的种类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4211b",
   "metadata": {},
   "source": [
    "#### PNANET模型\n",
    "GINPNA卷积提取的特征:  \n",
    "* 主邻域聚合：可能结合多种聚合函数(mean, max, sum等)  \n",
    "* 度感知放缩：考虑节点度进行特征放缩  \n",
    "* 多尺度信息：捕获不同范围的领域特征  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912706ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_class, **kargus):\n",
    "        super(PNANet, self).__init__()\n",
    "        ## --- 定义模型参数 ---\n",
    "        self.num_features = num_features    # 节点特征维度\n",
    "        self.num_class = num_class          # 节点的种类\n",
    "        self.dim = 32                       # 隐藏层维度\n",
    "        ## --- 第1层： GINPNA ---\n",
    "        nn1 = Sequential(Linear(self.num_features, self.dim), ReLU(), Linear(self.dim, self.dim))\n",
    "        self.conv1 = GINPNAConv()\n",
    "        ## --- 第2-5层： GINPNA ---\n",
    "        ## --- 全连接层 ---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
