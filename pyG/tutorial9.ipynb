{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f19dde6",
   "metadata": {},
   "source": [
    "## 教程9: 循环GNN\n",
    "在这个教程中,将实现图神经网络模型（不强制执行收缩映射）并分析pytorch geometric的GatedGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde48a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch.nn import Parameter as Param\n",
    "from torch import Tensor\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abad31",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4636c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Cora'\n",
    "path = osp.join('data', dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73e70e",
   "metadata": {},
   "source": [
    "### 图神经网络\n",
    "##### 多层感知机\n",
    "关键功能说明：  \n",
    "* 动态层构建：通过输入维度列表自动构建多层网络构建，避免硬编码  \n",
    "* 激活函数策略：仅在隐藏层添加Tanh激活，适合回归、特征转换任务  \n",
    "* 参数初始化：使用xavier_normal_初始化权重，缓解梯度消失、爆炸问题，提升训练稳定性  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58828990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # 构建层维度序列：[输入层, 隐藏层1, ... , 隐藏层n, 输出层]\n",
    "        dims = [input_dim] + hid_dim + [out_dim]\n",
    "        self.mlp = nn.Sequential()\n",
    "\n",
    "        # 动态构建网络层\n",
    "        for i in range(len(dims)-1):\n",
    "            # 添加线性层（全链接层）\n",
    "            self.mlp.add_module(f'lay_{i}', nn.Linear(dims[i], dims[i+1]))\n",
    "            # 除最后一层外都添加Tanh激活函数\n",
    "            if i+2 < len(dims):\n",
    "                self.mlp.add_module(f'act_{i}', nn.Tanh())\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"重置所有线性层的参数（Xavier正态初始化）\"\"\"\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0362baf",
   "metadata": {},
   "source": [
    "##### 图神经网络模型  \n",
    "关键功能说明：  \n",
    "* 节点状态管理： \n",
    "1. node_states是非可训练参数，用于存储节点特征迭代过程中的动态  \n",
    "2. 初始状态为0向量，后续通过消息传递更新 \n",
    "* 双MLP设计：  \n",
    "1. transition: 将聚合后的消息转化为新节点状态（维度不变），实现图上的状态传播\n",
    "2. readout: 将最终节点状态映射到分类输出空间（维度变化）\n",
    "* 收敛机制：通过eps控制迭代终止条件，避免固定层数的冗余计算.  \n",
    "$$\n",
    "x_{v}^{t+1} = f_{w}(l_{v}, l_{co(v)}, x^{t}_{ne(v)}, l_{ne(v)})\n",
    "$$\n",
    "$$\n",
    "o^{t}_{v} = g_w(x^{t}_{v}, l_{v})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddd6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNM(MessagePassing):\n",
    "    def __init__(self, n_nodes, out_channels, features_dim, hid_dims,\n",
    "                 num_layers=50, eps=1e-3, aggr='add', bias=True, **kwargs):\n",
    "        super(GNNM, self).__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        # 节点状态（非可训练参数）:存储每个节点的特征表示\n",
    "        self.node_states = nn.Parameter(torch.zeros((n_nodes, features_dim)), requires_grad=False)\n",
    "        self.out_channels = out_channels        # 分类类别\n",
    "        self.eps = eps                          # 收敛闸值\n",
    "        self.num_layers = num_layers            # 最大迭代次数\n",
    "\n",
    "        # 2个关键MLP\n",
    "        self.transition = MLP(features_dim, hid_dims, features_dim)     # 状态更新函数\n",
    "        self.readout = MLP(features_dim, hid_dims, out_channels)        # 输出函数\n",
    "\n",
    "        self.reset_parameters()\n",
    "        print(\"================transition==================\")\n",
    "        print(self.transition)\n",
    "        print(\"================readout==================\")\n",
    "        print(self.readout)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"重置MLP参数\"\"\"\n",
    "        self.transition.reset_parameters()\n",
    "        self.readout.reset_parameters()\n",
    "\n",
    "    def forward(self):\n",
    "        edge_index = data.edge_index    # 图结构（2, E） \n",
    "        edge_weight = data.edge_attr    # 边权重 (E, )\n",
    "        node_states = self.node_states\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # 1. 消息传递：聚合邻居消息\n",
    "            m = self.propagate(edge_index, x=node_states, edge_weight=edge_weight)\n",
    "            # 2. 状态更新：通过Transition MLP\n",
    "            new_states = self.transition(m)\n",
    "            # 3. 收敛检查\n",
    "            with torch.no_grad():\n",
    "                distance = torch.norm(new_states - node_states, dim=1)  # 计算：L2距离\n",
    "                convergence = distance < self.eps\n",
    "            node_states = new_states    # 更新节点状态\n",
    "            if convergence.all():       # 所有节点收敛则提前终止\n",
    "                break\n",
    "\n",
    "        # 4. 获取分类结果\n",
    "        out = self.readout(node_states)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "    \n",
    "    ## 计算从邻居j到中心节点的消息\n",
    "    def message(self, x_j, edge_weight):\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1)*x_j\n",
    "    \n",
    "    ## 使用稀疏矩阵乘法替代循环\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}({}, num_layers={})'.format(self.__class__.__name__,\n",
    "                                              self.out_channels, self.num_layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d3149f",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cb2379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================transition==================\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (act_0): Tanh()\n",
      "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_1): Tanh()\n",
      "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_2): Tanh()\n",
      "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_3): Tanh()\n",
      "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_4): Tanh()\n",
      "    (lay_5): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "================readout==================\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (act_0): Tanh()\n",
      "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_1): Tanh()\n",
      "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_2): Tanh()\n",
      "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_3): Tanh()\n",
      "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_4): Tanh()\n",
      "    (lay_5): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GNNM(data.num_nodes, dataset.num_classes, 32, [64,64,64,64,64], eps=0.01).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd6c3bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001, Train Acc:0.1286, Val Acc:0.1200, Val Acc:0.1290\n",
      "Epoch:002, Train Acc:0.1571, Val Acc:0.1420, Val Acc:0.1380\n",
      "Epoch:003, Train Acc:0.1429, Val Acc:0.1620, Val Acc:0.1490\n",
      "Epoch:004, Train Acc:0.1571, Val Acc:0.1140, Val Acc:0.1050\n",
      "Epoch:005, Train Acc:0.1643, Val Acc:0.0980, Val Acc:0.0950\n",
      "Epoch:006, Train Acc:0.1429, Val Acc:0.0580, Val Acc:0.0640\n",
      "Epoch:007, Train Acc:0.1429, Val Acc:0.0580, Val Acc:0.0640\n",
      "Epoch:008, Train Acc:0.1429, Val Acc:0.0580, Val Acc:0.0640\n",
      "Epoch:009, Train Acc:0.1500, Val Acc:0.0660, Val Acc:0.0690\n",
      "Epoch:010, Train Acc:0.1143, Val Acc:0.0740, Val Acc:0.0900\n",
      "Epoch:011, Train Acc:0.1357, Val Acc:0.0820, Val Acc:0.0920\n",
      "Epoch:012, Train Acc:0.1429, Val Acc:0.0800, Val Acc:0.0940\n",
      "Epoch:013, Train Acc:0.1214, Val Acc:0.0800, Val Acc:0.0990\n",
      "Epoch:014, Train Acc:0.1143, Val Acc:0.1180, Val Acc:0.1040\n",
      "Epoch:015, Train Acc:0.1571, Val Acc:0.1260, Val Acc:0.1340\n",
      "Epoch:016, Train Acc:0.1429, Val Acc:0.1680, Val Acc:0.1860\n",
      "Epoch:017, Train Acc:0.1571, Val Acc:0.2840, Val Acc:0.2830\n",
      "Epoch:018, Train Acc:0.1929, Val Acc:0.2820, Val Acc:0.2880\n",
      "Epoch:019, Train Acc:0.1571, Val Acc:0.2480, Val Acc:0.2280\n",
      "Epoch:020, Train Acc:0.2286, Val Acc:0.1540, Val Acc:0.1510\n",
      "Epoch:021, Train Acc:0.2286, Val Acc:0.1260, Val Acc:0.1160\n",
      "Epoch:022, Train Acc:0.2357, Val Acc:0.1360, Val Acc:0.1270\n",
      "Epoch:023, Train Acc:0.2286, Val Acc:0.0940, Val Acc:0.1060\n",
      "Epoch:024, Train Acc:0.2286, Val Acc:0.0940, Val Acc:0.1050\n",
      "Epoch:025, Train Acc:0.2429, Val Acc:0.0920, Val Acc:0.1060\n",
      "Epoch:026, Train Acc:0.2500, Val Acc:0.0980, Val Acc:0.1110\n",
      "Epoch:027, Train Acc:0.2571, Val Acc:0.1080, Val Acc:0.1210\n",
      "Epoch:028, Train Acc:0.2643, Val Acc:0.1240, Val Acc:0.1410\n",
      "Epoch:029, Train Acc:0.2500, Val Acc:0.1360, Val Acc:0.1500\n",
      "Epoch:030, Train Acc:0.2643, Val Acc:0.1420, Val Acc:0.1600\n",
      "Epoch:031, Train Acc:0.2429, Val Acc:0.1580, Val Acc:0.1560\n",
      "Epoch:032, Train Acc:0.2357, Val Acc:0.1520, Val Acc:0.1450\n",
      "Epoch:033, Train Acc:0.2500, Val Acc:0.1480, Val Acc:0.1420\n",
      "Epoch:034, Train Acc:0.2357, Val Acc:0.1460, Val Acc:0.1410\n",
      "Epoch:035, Train Acc:0.2357, Val Acc:0.1520, Val Acc:0.1480\n",
      "Epoch:036, Train Acc:0.2286, Val Acc:0.1340, Val Acc:0.1590\n",
      "Epoch:037, Train Acc:0.2286, Val Acc:0.1500, Val Acc:0.1560\n",
      "Epoch:038, Train Acc:0.2929, Val Acc:0.1340, Val Acc:0.1440\n",
      "Epoch:039, Train Acc:0.2643, Val Acc:0.1480, Val Acc:0.1540\n",
      "Epoch:040, Train Acc:0.2429, Val Acc:0.1600, Val Acc:0.1620\n",
      "Epoch:041, Train Acc:0.1786, Val Acc:0.0900, Val Acc:0.0980\n",
      "Epoch:042, Train Acc:0.1429, Val Acc:0.0720, Val Acc:0.0910\n",
      "Epoch:043, Train Acc:0.1429, Val Acc:0.0720, Val Acc:0.0910\n",
      "Epoch:044, Train Acc:0.1429, Val Acc:0.0720, Val Acc:0.0910\n",
      "Epoch:045, Train Acc:0.1429, Val Acc:0.0760, Val Acc:0.0970\n",
      "Epoch:046, Train Acc:0.1429, Val Acc:0.1560, Val Acc:0.1440\n",
      "Epoch:047, Train Acc:0.1429, Val Acc:0.1560, Val Acc:0.1440\n",
      "Epoch:048, Train Acc:0.1714, Val Acc:0.1740, Val Acc:0.1560\n",
      "Epoch:049, Train Acc:0.1571, Val Acc:0.1360, Val Acc:0.1210\n",
      "Epoch:050, Train Acc:0.1500, Val Acc:0.1020, Val Acc:0.1090\n"
     ]
    }
   ],
   "source": [
    "test_dataset = dataset[:len(dataset)//10]\n",
    "train_dataset = dataset[len(dataset)//10:]\n",
    "test_loader = DataLoader(test_dataset)\n",
    "train_loader = DataLoader(train_dataset)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item()/mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    train()\n",
    "    accs = test()\n",
    "    print(f'Epoch:{epoch:03d}, Train Acc:{accs[0]:.4f}, Val Acc:{accs[1]:.4f}, Val Acc:{accs[2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3280217",
   "metadata": {},
   "source": [
    "### 门控制图神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a197e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedGraphConv(MessagePassing):\n",
    "    def __init__(self, out_channels, num_layers, aggr='add', bias=True, **kwargs):\n",
    "        # 调用父类MessagePassing的初始化，aggr='add'表示求和聚合\n",
    "        super(GatedGraphConv, self).__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.out_channels = out_channels    # 存储输出维度  \n",
    "        self.num_layers = num_layers        # 存储迭代次数\n",
    "        # 创建可学习参数：[迭代次数， 输出维度， 输出维度]\n",
    "        self.weight = Param(Tensor(self.num_layers, self.out_channels, self.out_channels))\n",
    "        # 创建GRUCell单元：输入维度=out_channels, 隐藏维度=out_channels\n",
    "        self.rnn = torch.nn.GRUCell(self.out_channels, self.out_channels, bias=bias)\n",
    "        self.reset_parameters()     # 初始化参数\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # weight: uniform函数，从[-limit, limit]均匀分布初始化\n",
    "        uniform(self.out_channels, self.weight)\n",
    "        self.rnn.reset_parameters()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x                      # 获取节点特征，[num_nodes, in_channels]\n",
    "        edge_index = data.edge_index    # 边索引，[2, num_edge]\n",
    "        edge_weight = data.edge_attr    # 边权重，[num_edge]\n",
    "        # 1. 检查输入维度合法性-若输入维度>输出维度报错（无法通过填充0补充维度）\n",
    "        if x.size(-1) > self.out_channels:\n",
    "            raise ValueError('输入通道数不能大于输出通道数')\n",
    "            \n",
    "        # 2. 特征维度处理\n",
    "        if x.size(-1) < self.out_channels:\n",
    "            zero = x.new_zeros(x.size(0), self.out_channels-x.size(-1))\n",
    "            x = torch.cat([x, zero], dim=1)     # 补齐维度：[num_nodes, in_channels]->[num_nodes, out_channels]\n",
    "        \n",
    "        # 3. 迭代num_layers次（消息传递+GRU更新）\n",
    "        for i in range(self.num_layers):\n",
    "            # 3.1 线性变换： X @ W: [num_nodes, out_channels] @ [out_channels, out_channels] -> [num_nodes, out_cannels]\n",
    "            m = torch.matmul(x, self.weight[i])\n",
    "            # 3.2 消息传递, - message:生成边消息，-aggregate:聚合邻居消息\n",
    "            m = self.propagate(edge_index, x=m, edge_weight=edge_weight, size=None)\n",
    "            # 3.3 GRU更新:(输入消息m，隐藏状态x)-循环网络GRU-RNN [num_nodes, out_channels]\n",
    "            x = self.rnn(m, x)\n",
    "        # 4. 返回最终节点表示\n",
    "        return x\n",
    "    \n",
    "    # 消息构造函数\n",
    "    def message(self, x_j, edge_weight):\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "    \n",
    "    # 消息聚合函数\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"返回层的字符串表示\"\"\"\n",
    "        return f'''\n",
    "    {self.__class__.__name__}({self.out_channels}, num_layers={self.num_layers})\n",
    "    '''\n",
    "\n",
    "class GGNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GGNN, self).__init__()\n",
    "\n",
    "        self.conv = GatedGraphConv(1433, 3)\n",
    "        self.mlp = MLP(1433, [32, 32, 32], dataset.num_classes)\n",
    "    \n",
    "    def forward(self):\n",
    "        x = self.conv(data)\n",
    "        x = self.mlp(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541c704",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8433fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GGNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1c82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc:0.15000, Val Acc:0.16600, Test Acc:0.15200\n",
      "Epoch: 002, Train Acc:0.15000, Val Acc:0.14800, Test Acc:0.14300\n",
      "Epoch: 003, Train Acc:0.22857, Val Acc:0.21400, Test Acc:0.20400\n",
      "Epoch: 004, Train Acc:0.27857, Val Acc:0.23800, Test Acc:0.22200\n",
      "Epoch: 005, Train Acc:0.25714, Val Acc:0.24600, Test Acc:0.25100\n",
      "Epoch: 006, Train Acc:0.30000, Val Acc:0.27200, Test Acc:0.26300\n",
      "Epoch: 007, Train Acc:0.37143, Val Acc:0.32000, Test Acc:0.31500\n",
      "Epoch: 008, Train Acc:0.43571, Val Acc:0.36800, Test Acc:0.34200\n",
      "Epoch: 009, Train Acc:0.45000, Val Acc:0.41800, Test Acc:0.40100\n",
      "Epoch: 010, Train Acc:0.52143, Val Acc:0.46400, Test Acc:0.42600\n",
      "Epoch: 011, Train Acc:0.54286, Val Acc:0.47000, Test Acc:0.47800\n",
      "Epoch: 012, Train Acc:0.56429, Val Acc:0.47800, Test Acc:0.48900\n",
      "Epoch: 013, Train Acc:0.56429, Val Acc:0.53200, Test Acc:0.49600\n",
      "Epoch: 014, Train Acc:0.62857, Val Acc:0.51800, Test Acc:0.50900\n",
      "Epoch: 015, Train Acc:0.65000, Val Acc:0.52000, Test Acc:0.53000\n",
      "Epoch: 016, Train Acc:0.63571, Val Acc:0.53800, Test Acc:0.52100\n",
      "Epoch: 017, Train Acc:0.58571, Val Acc:0.51400, Test Acc:0.51900\n",
      "Epoch: 018, Train Acc:0.65000, Val Acc:0.53200, Test Acc:0.54600\n",
      "Epoch: 019, Train Acc:0.66429, Val Acc:0.51800, Test Acc:0.52900\n",
      "Epoch: 020, Train Acc:0.65000, Val Acc:0.51000, Test Acc:0.51600\n",
      "Epoch: 021, Train Acc:0.68571, Val Acc:0.54600, Test Acc:0.53800\n",
      "Epoch: 022, Train Acc:0.67857, Val Acc:0.56200, Test Acc:0.56400\n",
      "Epoch: 023, Train Acc:0.68571, Val Acc:0.57600, Test Acc:0.56100\n",
      "Epoch: 024, Train Acc:0.64286, Val Acc:0.57800, Test Acc:0.53700\n",
      "Epoch: 025, Train Acc:0.67143, Val Acc:0.55000, Test Acc:0.52500\n",
      "Epoch: 026, Train Acc:0.69286, Val Acc:0.57400, Test Acc:0.54200\n",
      "Epoch: 027, Train Acc:0.70714, Val Acc:0.58800, Test Acc:0.57100\n",
      "Epoch: 028, Train Acc:0.70714, Val Acc:0.61000, Test Acc:0.57800\n",
      "Epoch: 029, Train Acc:0.70714, Val Acc:0.60400, Test Acc:0.56600\n",
      "Epoch: 030, Train Acc:0.70714, Val Acc:0.60400, Test Acc:0.59200\n",
      "Epoch: 031, Train Acc:0.71429, Val Acc:0.60400, Test Acc:0.58400\n",
      "Epoch: 032, Train Acc:0.71429, Val Acc:0.59800, Test Acc:0.57400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accs\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     accs \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{:03d}\u001b[39;00m\u001b[38;5;124m, Train Acc:\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m, Val Acc:\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m, Test Acc:\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, accs[\u001b[38;5;241m0\u001b[39m], accs[\u001b[38;5;241m1\u001b[39m], accs[\u001b[38;5;241m2\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Data/software/conda/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataset = dataset[:len(dataset)//10]\n",
    "train_dataset = dataset[len(dataset)//10:]\n",
    "test_loader = DataLoader(test_dataset)\n",
    "train_loader = DataLoader(train_dataset)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item()/mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    train()\n",
    "    accs = test()\n",
    "    print('Epoch: {:03d}, Train Acc:{:.5f}, Val Acc:{:.5f}, Test Acc:{:.5f}'.format(epoch, accs[0], accs[1], accs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda6dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
