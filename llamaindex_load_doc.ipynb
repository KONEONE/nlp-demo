{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KONEONE/nlp-demo/blob/main/llamaindex_load_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA-WeJKB3__9"
      },
      "source": [
        "# LLAMAINDEX 加载 文档\n",
        "这篇笔记是记录llamaidnex加载文档的学习笔记  \n",
        "## 加载  \n",
        "* SimpleDirectoryReader： 用于本地目录加载各种文件类型的加载  \n",
        "* LlamaParse: LlammaIndex官方的PDF解析工具，作为托管API提供\n",
        "* LlamaHub: 包含了数百个数据加载库的注册中心  \n",
        "## 转换  \n",
        "* 节点解析器：  \n",
        "* 节点解析器模块：\n",
        "## 整合所有内容  \n",
        "设置一个可重复、缓存优化的加载数据的过程\n",
        "## 高级\n",
        "文档和节点对象以及高级使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beN9Bh5k3giI"
      },
      "outputs": [],
      "source": [
        "# 安装依赖\n",
        "%pip install llama-index-core\n",
        "%pip install llama-index-llms-dashscope\n",
        "%pip install llama-index-indices-managed-dashscope\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install llama-index-readers-file\n",
        "%pip install llama-index-vector-stores-qdrant\n",
        "%pip install qdrant-client\n",
        "%pip install llama-index-storage-kvstore-redis\n",
        "%pip install redis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install llama-index-node-parser-dashscope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-embeddings-dashscope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qCmsegNUFZZF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DASHSCOPE_API_KEY\"] = 'sk-a49fcd63a06f4b669c38b1461a60d2e4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6PrFTn32Fe4K"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.dashscope import DashScope\n",
        "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
        "\n",
        "# 将LlamaIndex的embeding model 设置为百炼\n",
        "Settings.embed_model = DashScopeEmbedding(model_name=\"text-embedding-v1\", api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
        "\n",
        "# 将LlamaIndex的llm model 设置为百炼\n",
        "Settings.llm = DashScope(model_name=\"qwen-plus\", api_key=os.getenv(\"DASHSCOPE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h40vwpMDD07Q"
      },
      "source": [
        "## 文档&节点 - 概述"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rpPT99_lD7TK"
      },
      "outputs": [],
      "source": [
        "### 根据文档构建索引\n",
        "from llama_index.core import Document, VectorStoreIndex\n",
        "\n",
        "test_list = [\"风急天高猿啸哀\", \"渚清沙白鸟飞回\", \"无边落木萧萧下\", \"不尽长江滚滚来\"]\n",
        "documents = [Document(text=t) for t in test_list]\n",
        "\n",
        "# 构建index\n",
        "indexs = VectorStoreIndex.from_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H7jrUHbEqEI"
      },
      "outputs": [],
      "source": [
        "### 根据Node构建索引\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# doc -段落切割-> node\n",
        "parser = SentenceSplitter()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# 构建node_index\n",
        "node_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNzUitMhH56R"
      },
      "source": [
        "### 文档使用  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iouataSaHzmt",
        "outputId": "fe51899d-37f2-4e51-a9e2-ad5a05ea99d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e754b69e-6308-4db2-ab87-15f27567d8fe - 4\n",
            "02fa5b3e-5e43-4e16-83e0-7193606ac8ec - 4\n",
            "19190085-cd83-4810-8ec6-418b95134c46 - 4\n",
            "e161aeb0-48dc-471f-bb4e-7ea048f48e01 - 4\n",
            "fb22b79b-7484-4a95-b9a2-0df092c95d23 - 4\n"
          ]
        }
      ],
      "source": [
        "### 加载文件生成doc\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "base_dir = \"./data/mkData\"\n",
        "baseDocuments = SimpleDirectoryReader(base_dir).load_data()\n",
        "\n",
        "for d in baseDocuments:\n",
        "  print(f'{d.id_} - {d.get_type()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0pYFsdHJB24",
        "outputId": "0c6663fb-0726-4d5e-dcc8-f2cf8079037f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27e89aa5-7057-427f-9e48-3d90fd5418bb - ObjectType.DOCUMENT\n",
            "cb671995-1bd6-461a-b5f9-6253ad325929 - ObjectType.DOCUMENT\n",
            "d40ddca0-dc4f-486c-baa8-7ef396133354 - ObjectType.DOCUMENT\n",
            "9a5e9433-6836-4f71-8a98-ef4efd4e6c7d - ObjectType.DOCUMENT\n"
          ]
        }
      ],
      "source": [
        "### 手动生成doc\n",
        "from llama_index.core import Document\n",
        "\n",
        "test_list = [\"风急天高猿啸哀\", \"渚清沙白鸟飞回\", \"无边落木萧萧下\", \"不尽长江滚滚来\"]\n",
        "testDocuments = [Document(text=t) for t in test_list]\n",
        "\n",
        "for d in testDocuments:\n",
        "  print(f'{d.id_} - {d.get_type()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlkZevpwOh6E"
      },
      "source": [
        "#### 自定义文档  \n",
        "任何在文档的metadata字典中设置的信息都会显示在该文档创建的每个源节点metadata中。这些信息包含在节点中，使索引能够在查询和响应中利用它。默认情况下，元数据会注入文本中，用于嵌入和LLM模型调用  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCpL9AUCOafV",
        "outputId": "8ef91e7a-f6f2-4183-b6fd-431c07ddcf25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'file_path': '/Volumes/Data/code/python_code/nlp-demo/data/mkData/gmall项目-DIM层.md', 'file_name': 'gmall项目-DIM层.md', 'file_size': 7, 'creation_date': '2024-03-04', 'last_modified_date': '2024-03-04'}\n",
            "{'file_path': '/Volumes/Data/code/python_code/nlp-demo/data/mkData/gmall项目-DWD层.md', 'file_name': 'gmall项目-DWD层.md', 'file_size': 9068, 'creation_date': '2024-03-04', 'last_modified_date': '2024-03-17'}\n",
            "{'file_path': '/Volumes/Data/code/python_code/nlp-demo/data/mkData/gmall项目-DWS层.md', 'file_name': 'gmall项目-DWS层.md', 'file_size': 7, 'creation_date': '2024-03-04', 'last_modified_date': '2024-03-04'}\n",
            "{'file_path': '/Volumes/Data/code/python_code/nlp-demo/data/mkData/gmall项目-ODS层.md', 'file_name': 'gmall项目-ODS层.md', 'file_size': 6734, 'creation_date': '2024-03-04', 'last_modified_date': '2024-03-06'}\n",
            "{'file_path': '/Volumes/Data/code/python_code/nlp-demo/data/mkData/图布局算法.md', 'file_name': '图布局算法.md', 'file_size': 2367, 'creation_date': '2024-12-30', 'last_modified_date': '2024-12-30'}\n"
          ]
        }
      ],
      "source": [
        "for d in baseDocuments:\n",
        "  print(f'{d.metadata}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Awxea9Qszf"
      },
      "source": [
        "### 使用Node  \n",
        "在llamaindex中构建节点关系（relationships）是构建结构化知识图谱的关键机制，它使得节点之间形成有意义的连接，这些关系对如下场景非常重要：  \n",
        "1. 保持文档结构  \n",
        "1.1 当原始文档被拆分为多个节点时，关系可以重建原始顺序  \n",
        "1.2 Next/Previous关系维护文本的连续性，就像链表一样链接节点  \n",
        "2. 增强索引质量  \n",
        "2.1 索引时可以获取相关节点（如前\\后文）提供更加完整的上下文    \n",
        "2.2 示例：回答问题时，获取匹配节点的前后文能提高答案准确性  \n",
        "3. 构建知识图谱  \n",
        "3.1 表示概念间的语义关系（如父子、引用等）  \n",
        "3.2 PARENT、CHILD关系可以构建层次结构  \n",
        "4. 图遍历能力  \n",
        "4.1 支持复杂查询  \n",
        "4.2 实现多跳推理  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0CLz3gbQsTi"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
        "\n",
        "# 创建2个连续文本带你\n",
        "node1 = TextNode(text=\"段落1\", id=\"node01\")\n",
        "node2 = TextNode(text=\"段落2\", id=\"node02\")\n",
        "\n",
        "# 创建双向连接关系\n",
        "node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=\"node02\")  # node1 下一个节点：node2\n",
        "node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=\"node01\") # node2 下一个节点： node1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FK17mQEjRWH"
      },
      "source": [
        "下面给出了NodeRelationship的一些常用关系：\n",
        "\n",
        "|关系类型 |\t说明\t| 典型应用|  \n",
        "|----|----|----|  \n",
        "|NEXT\t|下一个节点|\t连续文本|\n",
        "|PREVIOUS\t|前一个节点\t|连续文本|\n",
        "|PARENT\t|父节点\t|章节结构|\n",
        "|CHILD\t|子节点\t|章节结构|\n",
        "|SOURCE\t|源文档\t|节点溯源|\n",
        "|REFERENCE\t|引用节点\t|跨文档链接|\n",
        "|CONTEXT\t|相关上下文\t|补充信息|\n",
        "\n",
        "\n",
        "### 元数据提取  \n",
        "元数据模块包含以下“特征提取器”：  \n",
        "* SummaryExtractor : 自动从一组节点中提取摘要  \n",
        "* QuestionsAnsweredExtractor: 提取每个接待您可以回答的一组问题  \n",
        "* TitleExtractor: 从每个节点的上下文提取标题\n",
        "* EntityExtractor: 提取每个节点内容中提到的实体（地点、人物、事务等）  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "963ffe4adc4a46e397c0053a2641462e",
            "3010163abc39461383aff863b16897dd",
            "6582f70cfdf64bc5861deb4c13381638",
            "812cfecbb2b14b0db18ab3cfdf17821e",
            "545c498e96e64cc89731b2c2af6bcc3b",
            "28809eedd40e4d47bc82bd74ffd3dad9",
            "c7a1870238c047d6b1560e29d90360a9",
            "d2f49382e7cd4b6d8c51d540435e6326",
            "e9748cd9e5924bcf993c3223b8c5edb4",
            "1a1227ef503d43be9b4db578ecf21317",
            "9c62252459c543efa59cfbeabf2b1b0d"
          ]
        },
        "id": "aL-lmjMnjKVe",
        "outputId": "c2bc1a4d-e051-4b05-e65f-5e78f0448f87"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "963ffe4adc4a46e397c0053a2641462e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:32<00:00,  4.63s/it]\n",
            "100%|██████████| 110/110 [04:38<00:00,  2.53s/it]\n"
          ]
        }
      ],
      "source": [
        "# 导入元数据提取器和节点解析器\n",
        "from llama_index.core.extractors import (\n",
        "    TitleExtractor, QuestionsAnsweredExtractor\n",
        ")\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# 1. 创建文本分割器（节点解析器）\n",
        "text_splitter = TokenTextSplitter(\n",
        "    separator=\" \",      # 使用空格作为分隔符\n",
        "    chunk_size=512,     # 每个节点的最大token数量\n",
        "    chunk_overlap=128   # 节点间重叠token数量\n",
        ")\n",
        "\n",
        "# 2. 创建元数据提取器\n",
        "#   2.1 自动生成节点内容的标题\n",
        "title_extractor = TitleExtractor(\n",
        "    nodes = 5  # 使用上下5个节点生成上下文标题\n",
        ")\n",
        "#   2.2 提取节点能回答的问题\n",
        "qa_extractor = QuestionsAnsweredExtractor(\n",
        "    questions=3  # 为每个节点生成能回答的3个问题\n",
        ")\n",
        "\n",
        "# 3. 创建数据处理管道\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,    # 步骤1：分割文档\n",
        "        title_extractor,  # 步骤2：提取标题\n",
        "        qa_extractor      # 步骤3：生成问题\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. 运行管道处理文档\n",
        "#   4.1 第一种写法\n",
        "pipelineNodes = pipeline.run(\n",
        "    documents=baseDocuments,  # 输入docs\n",
        "    in_place=True,        # 直接修改文档对象\n",
        "    show_progress= True,  # 显示进度条\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HOOkKTlovHI",
        "outputId": "c897824e-8153-416a-89f8-eea2455835fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d6bbf91f-433f-4970-9847-2e06e730848b ->  数据库设计与SQL优化实战：从范式理论到AWS IAM跨账号安全访问  \n",
            "\n",
            "这个标题既涵盖了数据库设计与SQL查询优化的核心内容，也体现了AWS IAM跨账号访问的实际应用场景，体现了文档的技术深度与广度。\n",
            "18e9063c-2fae-4c84-b78a-a0b2e678eb61 ->  数据库设计与SQL优化实战：从范式理论到AWS IAM跨账号安全访问  \n",
            "\n",
            "这个标题既涵盖了数据库设计与SQL查询优化的核心内容，也体现了AWS IAM跨账号访问的实际应用场景，体现了文档的技术深度与广度。\n",
            "8e08e254-0c38-45c0-837c-7a4dd46c92f2 ->  数据库设计与SQL优化实战：从范式理论到AWS IAM跨账号安全访问  \n",
            "\n",
            "这个标题既涵盖了数据库设计与SQL查询优化的核心内容，也体现了AWS IAM跨账号访问的实际应用场景，体现了文档的技术深度与广度。\n",
            "939ed30a-9de3-496c-91a3-34cd14eae3e0 ->  数据库设计与SQL优化实战：从范式理论到AWS IAM跨账号安全访问  \n",
            "\n",
            "这个标题既涵盖了数据库设计与SQL查询优化的核心内容，也体现了AWS IAM跨账号访问的实际应用场景，体现了文档的技术深度与广度。\n"
          ]
        }
      ],
      "source": [
        "for n in pipelineNodes[:4]:\n",
        "  print(f'{n.id_} -> {n.metadata[\"document_title\"].split(\"**Title:\")[1].replace(\"**\", \"\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppKaoGL5wFEw"
      },
      "outputs": [],
      "source": [
        "#   4.2 第二种写法\n",
        "from llama_index.core import VectorStoreIndex\n",
        "nodes_index = VectorStoreIndex.from_documents(\n",
        "    documents, transformations=[text_splitter, title_extractor, qa_extractor]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSxpAjiDxRVK"
      },
      "source": [
        "## 节点解析器、文本分割器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-41gTuRxQfE"
      },
      "outputs": [],
      "source": [
        "# 单独使用\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "node_parser = SentenceSplitter(\n",
        "    chunk_size=1024,    # 块大小\n",
        "    chunk_overlap=256,  # 上下块覆盖大小\n",
        ")\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(\n",
        "    documents=baseDocuments,    # 导入doc\n",
        "    show_progress=True,         # 显示处理进度\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcvhlKKBzpIY"
      },
      "outputs": [],
      "source": [
        "# 转换使用\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "# 定义pipeline的操作序列\n",
        "pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n",
        "\n",
        "# 执行nodes\n",
        "nodes = pipeline.run(documents=baseDocuments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNGIg2gX2M0l"
      },
      "source": [
        "### 索引使用   \n",
        "设置在transformations或全局设置中，以便在通过 .from_documents() 构建索引时自动使用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je67mY-o49yE"
      },
      "source": [
        "# 数据管道\n",
        "输入数据中，这些Transformations会被应用到输入的数据中，最终生成node。这些node要么被返回要么被插入到向量数据库中。     \n",
        "下面是简单的使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF9h-RWK49Kg"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
        "\n",
        "# 创建pipeline转化\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=0),   # 定义划分node\n",
        "        TitleExtractor(),     # 给出 title\n",
        "        OpenAIEmbedding(),    # 创建 embedding\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 生成node\n",
        "tmpNode = pipeline.run(documents=baseDocuments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Hwslgl7Rxn"
      },
      "source": [
        "连接到向量数据库  \n",
        "当输入数据被转换后，将embedding的node插入到远程向量数据库中。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLpmphWy7uGU"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "import qdrant_client\n",
        "\n",
        "# 创建Qdrant向量数据库\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "# 创建Qdrant向量存储实例\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
        "\n",
        "# 创建数据处理管道\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=256),   # 文本分割\n",
        "        TitleExtractor(),       # 提取块的标题\n",
        "        OpenAIEmbedding(),      # 生成embedding\n",
        "    ],\n",
        "    vector_store=vector_store   # 指定存储的位置\n",
        ")\n",
        "\n",
        "# 对数据进行处理\n",
        "pipeline.run(documents=baseDocuments)\n",
        "\n",
        "# 从向量数据库中创建index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# 直接从数据库中创建索引\n",
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g61pjKWjPor1"
      },
      "source": [
        "### 管道缓存  \n",
        "在IngestionPipeline中，可以缓存pipeline,这样可以节省后续使用相同数据转换的操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnXoen43PrgO"
      },
      "outputs": [],
      "source": [
        "# 存储pipeline\n",
        "pipeline.persist(f\"./pipeline_storage\")\n",
        "# 加载pipeline\n",
        "new_pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=1024, chunk_overlap=256),   # 文本分割\n",
        "        TitleExtractor(),       # 提取块的标题\n",
        "        OpenAIEmbedding(),      # 生成embedding\n",
        "    ],\n",
        "    vector_store=vector_store   # 指定存储的位置\n",
        ")\n",
        "new_pipeline.load(\"./pipeline_storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdUHU_7lQnD1"
      },
      "source": [
        "下面将pipeline数据存储到redis中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYBaLSYsQqUM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
        "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
        "\n",
        "# 实例化 redis Cache\n",
        "ingest_cache = IngestionCache(\n",
        "    cache = RedisCache.from_host_and_port(host=\"localhost\", port=6379),\n",
        "    collection = \"my_test_cache\"\n",
        ")\n",
        "\n",
        "# 存储到redis中\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "      SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
        "      TitleExtractor(),\n",
        "      OpenAIEmbedding(),\n",
        "    ],\n",
        "    cache=ingest_cache,\n",
        ")\n",
        "\n",
        "redis_cache_nodes = pipeline.run(documents=baseDocuments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3_j7ZEXT63I"
      },
      "source": [
        "# 使用属性图索引\n",
        "属性图是一种知识集合，由带标签的节点（即实体类别、文本标签等）组成，这些节点具有属性（即元数据），并通过关系连接成结构化的路径。  \n",
        "### SimpleLLMPathExtractor 三元组提取器\n",
        "SimpleLLMPathExtractor是知识三元组提取器，核心就是从非结构化的文本中，自动抽取结构化的知识。并且这些知识以“主语，谓语，宾语”的形式  \n",
        "主要功能如下：  \n",
        "* 自动化知识提取  \n",
        "* 结构化输出  \n",
        "* 高度可定制化  \n",
        "* 集成于LlamaIndex工作流  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fBQrNpBcRidw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
        "# 定义一个提示词模板，用于指导LLLM如何从文本中提取知识三元组\n",
        "prompt = (\n",
        "    \"Some text is provided below. Given the text, extract to\"\n",
        "    \"{max_path_per_chunk}\"  # 这是一个占位符，会被实际数值替换\n",
        "    \"knowledge triples in the form of `subject,predicate,object` on each line. Avoid stopwords.\\n\"\n",
        "    # 提示词要求LLM以“注意，谓词，客体”的格式提取知识三元组，每行一个，并且避免使用停用词\n",
        ")\n",
        "\n",
        "# 定义一个解析函数，用于处理LLM响应并将其转化为三元组\n",
        "def parse_fn(response_str:str) -> list[(str, str, str)]:\n",
        "    # 将相应字段按照换行符分割为多行\n",
        "    lines = response_str.split(\"\\n\")\n",
        "    # 对每行按照逗号，分割为三元组\n",
        "    triples = [line.split(\",\") for line in lines]\n",
        "    return triples\n",
        "\n",
        "\n",
        "# 创建知识图谱提取器实例\n",
        "kg_extractor = SimpleLLMPathExtractor(\n",
        "    extract_prompt=prompt,\n",
        "    parse_fn=parse_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ImplicitPathExtractor\n",
        "这个提取器不需要 LLM 或嵌入模型来运行，因为它只是解析 llama-index 节点对象上已存在的属性 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.indices.property_graph import ImplicitPathExtractor\n",
        "\n",
        "kg_imp_extractor = ImplicitPathExtractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
        "\n",
        "kg_dm_extractor = DynamicLLMPathExtractor(\n",
        "    max_triplets_per_chunk=20,      # 限制每一段文本块中最多提取20个三元组，这有利于控制输出数量和质量\n",
        "    num_workers=4,\n",
        "    # 【核心功能：实体类型过滤】\n",
        "    #   指定允许提取的实体类型（主语和宾语的类型）\n",
        "    #   这是一个强大的过滤机制，提取器会优先识别和提取属于这些类型的实体\n",
        "    allowed_entity_types=[\"POLITICIAN\", \"POLITICAL_PARTY\"],\n",
        "    # 【核心功能：关系类型过滤】\n",
        "    #   指定允许提取的关系类型（谓词种类）\n",
        "    #   提取器只关注并提取这些特定的关系\n",
        "    allowed_relation_types=[\"PRESIDENT_OF\", \"MEMBER_OF\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
        "\n",
        "\n",
        "# 定义知识图谱中所有的节点类型\n",
        "entities = Literal['PERSON', 'PLACE', 'THING']\n",
        "# 定义知识图谱中所有的边类型\n",
        "relations = Literal['PART_OF', 'HAS', 'IS_A']\n",
        "\n",
        "# 定义一个字典，精确定义了哪些类型的实体之间可以存在哪些类型的关系\n",
        "# 这是核心配置，定义了知识图谱的结构\n",
        "schema = {\n",
        "    # key: 实体类型\n",
        "    # value: 该类型实体可以作为主语拥有的关系类型列表\n",
        "    \"PERSON\": [\"PART_OF\", \"HAS\", \"IS_A\"], # 含义：人(PERSON)可以是yyy的一部分(PART_OF),可以拥有(HAS)xxx,可以是hhh的一种(IS_A)\n",
        "    \"PLACE\": [\"PART_OF\", \"HAS\"], # 地方(PLACE)可以是xxx的一部分(PART_OF),可以拥有(HAS)xxx\n",
        "    \"THING\": [\"IS_A\"], # 东西(THING)可以是yyy的一种(IS_A)\n",
        "}\n",
        "\n",
        "kg_sc_extractor = SchemaLLMPathExtractor(\n",
        "    llm=Settings.llm,\n",
        "    possible_entities=entities,\n",
        "    possible_relations=relations,\n",
        "    kg_validation_schema=schema,\n",
        "    # 【核心参数：严格模式】\n",
        "    #   1. 如果设置True，提取器只保留符合kg_validation_schema的三元组\n",
        "    #   2. 如果设置False，不符合kg_validation_schema的也会被保留，但是符合kg_validation_schema的会被优先提取\n",
        "    strict=True,\n",
        "    num_workers=4,\n",
        "    max_triplets_per_chunk=10,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 案例\n",
        "下面是一个graph的简单案例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1. 索引\n",
        "from llama_index.core import PropertyGraphIndex\n",
        "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
        "\n",
        "graph_index = PropertyGraphIndex.from_documents(\n",
        "    baseDocuments,\n",
        "    llm = s\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOShufRZd04GL7n/4fUL37y",
      "include_colab_link": true,
      "mount_file_id": "1hWQ3DTlsKywFHDGJs2pPegwc5pf9fr0S",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "dl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a1227ef503d43be9b4db578ecf21317": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28809eedd40e4d47bc82bd74ffd3dad9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3010163abc39461383aff863b16897dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28809eedd40e4d47bc82bd74ffd3dad9",
            "placeholder": "​",
            "style": "IPY_MODEL_c7a1870238c047d6b1560e29d90360a9",
            "value": "Parsing nodes: 100%"
          }
        },
        "545c498e96e64cc89731b2c2af6bcc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6582f70cfdf64bc5861deb4c13381638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f49382e7cd4b6d8c51d540435e6326",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9748cd9e5924bcf993c3223b8c5edb4",
            "value": 7
          }
        },
        "812cfecbb2b14b0db18ab3cfdf17821e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a1227ef503d43be9b4db578ecf21317",
            "placeholder": "​",
            "style": "IPY_MODEL_9c62252459c543efa59cfbeabf2b1b0d",
            "value": " 7/7 [00:00&lt;00:00, 21.47it/s]"
          }
        },
        "963ffe4adc4a46e397c0053a2641462e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3010163abc39461383aff863b16897dd",
              "IPY_MODEL_6582f70cfdf64bc5861deb4c13381638",
              "IPY_MODEL_812cfecbb2b14b0db18ab3cfdf17821e"
            ],
            "layout": "IPY_MODEL_545c498e96e64cc89731b2c2af6bcc3b"
          }
        },
        "9c62252459c543efa59cfbeabf2b1b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7a1870238c047d6b1560e29d90360a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f49382e7cd4b6d8c51d540435e6326": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9748cd9e5924bcf993c3223b8c5edb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
